{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJ7s_SIVu_I"
      },
      "source": [
        "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
        "\n",
        "### Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "### Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "### LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "### Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "### Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Incio notas santi \n",
        "-----------------------------------------------------------------\n",
        "\n",
        "https://scikit-learn.org/stable/modules/lda_qda.html\n",
        "https://www.youtube.com/watch?v=azXCzI57Yfc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TDWOgpJWKQa"
      },
      "source": [
        "## Estructura del código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yEV8WbiWl6k"
      },
      "source": [
        "## Modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "teF9O9JJmG7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sDBLvbTtlwzs"
      },
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "  def fit(self, y):\n",
        "    self.names = np.unique(y) #nombres unicos\n",
        "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)} #diccionario, para cada nombre un numero\n",
        "    self.fmt = y.dtype #esto solo sirve para crear un array del mismo tipo de dato\n",
        "    \n",
        "    # Q1: why is there no need for class_to_name?\n",
        "    \"\"\"\n",
        "    Por que las tags del data set original estan definidas como nombres (strings). \n",
        "    Super ineficiente usar strings. \n",
        "    \n",
        "    //chequear si usa la clase para una operacion\n",
        "    si se usa para indexar matrices del data set que corresponden a una misma clase. \n",
        " \n",
        " \n",
        "    \"\"\"\n",
        "\n",
        "  def _map_reshape(self, f, arr):\n",
        "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "    # Q2: why is reshaping necessary?\n",
        "    \"\"\"\n",
        "    Flatten para que sea generico el metodo. \n",
        "    Para que tenga el mismo shape que el vector de etiquetas que le voy a pasar para que haga el encoder\n",
        "    al fin y al cabo lo que quiero  hacer es hacer un encoder de los nombres, no cambiar morfologia. \n",
        "    ademas ese flaten me reshapeea en una dimension renglon.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  def transform(self, y):\n",
        "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "  def fit_transform(self, y):\n",
        "    self.fit(y)\n",
        "    return self.transform(y)\n",
        "\n",
        "  def detransform(self, y_hat):\n",
        "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m0KYC8_uSOu4"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    self.encoder = ClassEncoder()\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: what does bincount do?\n",
        "    \"\"\"\n",
        "    bincount: Counts number of occurrences of each value in array of non-negative ints.  \n",
        "    ese a_priori lo que hace es calcular las frecuencias relativas de cada clase en el dataset.  \n",
        "    o sea este estimate a priori saca basicamente lo que vendria a ser un histograma....   \n",
        "    y el bincount cuenta los elementos de y pero aplastados en 1 renglon. \n",
        "    solo acepta ints asi que debe usar algo que paso por el encoder\n",
        "\n",
        "    le pegue una chequeada y te tira esto [0.33333333 0.33333333 0.33333333]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return np.log(a_priori)\n",
        "    \n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # first encode the classes\n",
        "    y = self.encoder.fit_transform(y)\n",
        "\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # check that a_priori has the correct number of classes\n",
        "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: why do we need to do this last, can't we do it first?\n",
        "    \"\"\"\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "    sin encoder no se puede indexar con I (tendrias strings)\n",
        "    Sin estimar a priori, esta funcion tampoco se podria evaluar len(self.log_a_priori)  (si bien el assert te permitiria usar encoder names)\n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "    \n",
        "    for i in range(m_obs):\n",
        "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i \n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IRamFdiGDuSR"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate COMMON  covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: why not just X[:,y==idx]?\n",
        "    # Inv_covs tiene tres matrices de covarianza en principio, distintas.\n",
        "    # Matriz de covarianza es de 4x4 features \n",
        "    # para indexar se necesita un vector unidimensional, y el vector y tiene forma de matriz de 90x1. \n",
        "\n",
        "\n",
        "    # Q6: what does bias=True mean? why not use bias=False?\n",
        "    \n",
        "    \"\"\"\n",
        "    np.cov: Estimate a covariance matrix, given data and weights.\n",
        "    bias: Default normalization (False) is by (N - 1), where N is the number of observations given (unbiased estimate). \n",
        "    If bias is True, then normalization is by N.  1/N   \n",
        "\n",
        "    Sabemos que las poblaciones tienen distibucion normal por hipotesis\n",
        "    Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "    Y sabemos que para esos casos un estimador de maximum liklyhhod para la \n",
        "    covarianza de pobalcions que siguen distribucion normal tiene un factor de normalizacion de tipo 1/N\n",
        "\n",
        "    In cases where the distribution of the random variable X is known to be within a certain family of distributions, \n",
        "    other estimates may be derived on the basis of that assumption. A well-known instance is when the random variable X \n",
        "    is normally distributed: in this case the maximum likelihood estimator of the covariance matrix is slightly different \n",
        "    from the unbiased estimate, and is given by \n",
        "\n",
        "\n",
        "       \n",
        "    https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices\n",
        "    \"\"\"\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: what does axis=1 mean? why not axis=0 instead?\n",
        "\n",
        "    # significa que voy a calcular la media en direccion de los renglones , eporque los datos estan en filas luego de hacer \n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "    \n",
        "    # el indexado me devuelve la sub matriz que corresponde a muestras de una misma clase. y sobre eso le calculo una media\n",
        "\n",
        "    # es decir que obtenes la media de los 4 features para cada clase. el vector de medias es una lista de 3 elementos donde cada elemento tiene 4 componentes. \n",
        "\n",
        "    \n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_zoK-gWkRf"
      },
      "source": [
        "## Código para pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m05KrhUDINVs"
      },
      "outputs": [],
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6667"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkXcoldXOqs",
        "outputId": "b07a5027-be83-4c0a-a09e-e4f3a21e4c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "X_full, y_full = get_iris_dataset()\n",
        "\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAk-UQCjKecT",
        "outputId": "ddf4e2f6-1baf-4a51-de72-5ce1d7c03db8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek data matrix\n",
        "X_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdzMURX2KVdO",
        "outputId": "66a3cd6b-7dda-4618-b13f-628d113bf7d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa']], dtype='<U10')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek target vector\n",
        "y_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP_QmWCIECs",
        "outputId": "36c28bcc-5d33-43e6-f231-3f3bf7b460cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "# preparing data, train - test validation\n",
        "# 70-30 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "\n",
        "# traspose everything because this model format assumes column vectors\n",
        "train_x = X_train.T\n",
        "train_y = y_train.T\n",
        "test_x = X_test.T\n",
        "test_y = y_test.T\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dGIf2TA5SpoT"
      },
      "outputs": [],
      "source": [
        "qda = QDA()\n",
        "\n",
        "qda.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Q30DyLWpTL",
        "outputId": "c113c448-5230-44be-8f85-7a6d3f732d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0000 while test error is 0.0333\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yb1V7_yXRfO"
      },
      "source": [
        "# Consigna\n",
        "\n",
        "## Implementación\n",
        "1. Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias?¿Por qué?\n",
        "2. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
        "1. *(Opcional)* En `BaseBayesianClassifier._predict_one` se estima la posteriori de cada clase por separado, a pesar de que la cuenta es siempre la misma (cambiando valores de parámetros, pero no dimensiones). Aprovechando el *broadcasting* de NumPy, se puede reemplazar ese list comprehension por un cálculo *tensorizado* donde tanto medias como varianzas (o inversas) estén \"stackeadas\" sobre un tercer eje, permitiendo el cálculo en paralelo de todas las clases con un:\n",
        "`log_posteriori = self.tensor_log_a_priori + self._predict_log_conditionals(x)`. Implementar dicha modificación y mostrar su uso. *Ayuda: los métodos `np.stack` y la documentación del operador [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) son de gran utilidad.*\n",
        "\n",
        "## Preguntas técnicas\n",
        "\n",
        "Responder las 7 preguntas que se encuentran distribuidas a lo largo del código.\n",
        "\n",
        "## Preguntas teóricas\n",
        "\n",
        "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
        "\n",
        "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
        "\n",
        "3. La implementación de QDA estima la probabilidad condicional utilizando 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x que no es exactamente lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preguntas teoricas\n",
        "\n",
        "### 1- Derivación de LDA\n",
        "\n",
        "Dado que para LDA $\\Sigma_j == \\Sigma $:\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva. Luego, distribuyendo las matrices resultantes:\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2} x^T \\Sigma^{-1} x + \\frac{1}{2} x^T \\Sigma^{-1} \\mu_j + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C \n",
        "$$\n",
        "\n",
        "El primer término de la suma es independiente de la clase j, por lo que se puede incorporar a la constante aditiva.\n",
        "Ahora, si tomamos el segundo término de la suma, usando la propiedad de la transpuesta de un producto de matrices $ (AB)^T = B^T A^T $ y que $ (A^T)^T = A $ , resulta que \n",
        "$$\n",
        "x^T \\Sigma^{-1} \\mu_j = ( \\mu_j^T (\\Sigma^{-1})^T x )^T\n",
        "$$\n",
        "Dado que la matríz $\\Sigma$ es cuadrada y simétrica, luego $\\Sigma ^ -1$ es cuadrada y simétrica, entonces $ (\\Sigma^{-1})^T = (\\Sigma^{-1})$. Por lo tanto, la ecuación para el logaritmo de la probabilidad resulta:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\frac{1}{2} [\\mu_j^T \\Sigma^{-1} x ]^T + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C \n",
        "$$\n",
        "\n",
        "Ahora, viendo las dimensiones de las matrices del primer término, tenemos que $\\mu_j \\epsilon ℝ^{(1xk)}$,  $\\Sigma \\epsilon ℝ^{(kxk)}$ y  $x \\epsilon ℝ^{(kx1)} $, donde k es la cantidad de features del dataset.\n",
        "\n",
        "Por lo tanto $ [\\mu_j^T \\Sigma^{-1} x ] $ es un escalar y la ecuación se puede reducir de la siguiente forma:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C = \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j^T) + C\n",
        "$$\n",
        "\n",
        "Lo que resulta que, para LDA:\n",
        "$$\n",
        "\\log{f_j(x)} = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j^T) + C\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 2- Quadratic vs Linear\n",
        "\n",
        "Como se demostro anteriormente, la funcion a maximizar de LDA tiene un termino lineal. \n",
        "\n",
        "LDA tiene solo terminos lineales en su funcion a maximizar. \n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "Sin embargo, QDA tiene un termino cuadratico, que no se puede simplificar como en LDA porque la matriz de covarianza depende de la clase\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "\n",
        "### 3-  Propiedades de los logaritmos\n",
        "\n",
        "En la parte teórica, se presenta la siguiente fórmula para QDA:\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "Sin embargo, el codigo provisto usa la inversa de matriz de covarianza: `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x`\n",
        "\n",
        "Estas dos formas de calcular el resultado son equivalentes ya que, $ |\\Sigma_j| = \\frac{1}{|\\Sigma_j^{-1}|} = (|\\Sigma_j^{-1}|)^{-1} $ y por lo tanto el primer término de la fórmula teórica equivale a $  -\\frac{1}{2}\\log |\\Sigma_j| = +\\frac{1}{2}\\log |\\Sigma_j^{-1}| $\n",
        "\n",
        "En la implementación práctica se prioriza utilizar la variable $ \\Sigma_j^{-1} $ ya que de esta forma se evita tener que guardar el valor tanto de $ \\Sigma_j $ como de $ \\Sigma_j^{-1} $.\n",
        "\n",
        "# SUBIR!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementacion punto 1\n",
        "#### 1) Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias? ¿Por qué?\n",
        "\n",
        "No se encontraron diferencias al entrenar un modelo de QDA con una distribución uniforme en comparación con el modelo original provisto.\n",
        "\n",
        "Esto se debe a que, en el caso de no proveer explicitamente una distribución a_priori cuando se llama al método QDA.fit(), la clase QDA computa automáticamente la distribución a_priori como la proporción de cada clase dentro del dataset. Para el caso del dataset \"iris\", el mismo se encuentra perfectamente balanceado, habiendo 50 muestras para cada clase. Por lo tanto, dado que el código original no establece una distribución apriori distinta, el cálculo automático de QDA.fit() termina computando una distribución uniforme dado que la proporción de cada clase dentro del dataset es la misma.\n",
        "\n",
        "\n",
        "# SUBIR !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## implementacion punto 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1ChynN-GXSL5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate COMMON  covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True)) \n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    \n",
        "\n",
        "    total = len(y)  # ejemplo 90\n",
        "    (unique, counts) = np.unique(y, return_counts=True)  # (0, cantidad de ceros) (1, cantidad de 1) (2, cantidad de 2)\n",
        "    self.inv_cov_inbetween = np.zeros(self.inv_covs[0].shape)\n",
        "\n",
        "    #asumo que las matrices de covariaza estan ordenadas por clase. entonces\n",
        "    for index in range(len(counts)):\n",
        "      self.inv_cov_inbetween = self.inv_cov_inbetween + np.multiply( self.inv_covs[index], (counts[index])/total )\n",
        "\n",
        "    print(np.shape(self.inv_cov_inbetween))\n",
        "    self.inv_covs = [self.inv_cov_inbetween for i in range(len(unique))]\n",
        "\n",
        "\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(det(np.linalg.inv( inv_cov))) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 4)\n"
          ]
        }
      ],
      "source": [
        "un_lda = LDA()\n",
        "\n",
        "un_lda.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparacion de metodos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDA Train (apparent) error is 0.00000000 while test error is 0.03333333\n",
            "LDA Train (apparent) error is 0.02222222 while test error is 0.03333333\n"
          ]
        }
      ],
      "source": [
        "def accuracy_qda(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"QDA Train (apparent) error is {1-train_acc:.8f} while test error is {1-test_acc:.8f}\")\n",
        "\n",
        "\n",
        "def accuracy_lda(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, un_lda.predict(train_x))\n",
        "test_acc = accuracy(test_y, un_lda.predict(test_x))\n",
        "print(f\"LDA Train (apparent) error is {1-train_acc:.8f} while test error is {1-test_acc:.8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
        "\n",
        "Se usaron valores de random seed:  0 , 42,  425, 6543, 888, 6667\n",
        "\n",
        "Los valores obtenidos fueron: \n",
        "\n",
        "\n",
        "\n",
        "Puede observarse que tanto los modelos lineales como cuadraticos tienen el mismo error de test en ambos casos. pero distintos errores de test"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AMIA - TP final LDA/QDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "54dee8e2628c7a348348274027baca590104374d7a70a03370ff5ec22cf94c02"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
