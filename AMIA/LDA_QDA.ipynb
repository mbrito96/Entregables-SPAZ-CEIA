{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJ7s_SIVu_I"
      },
      "source": [
        "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
        "\n",
        "### Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "### Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "### LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "### Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "### Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Incio notas santi \n",
        "-----------------------------------------------------------------\n",
        "\n",
        "https://scikit-learn.org/stable/modules/lda_qda.html\n",
        "https://www.youtube.com/watch?v=azXCzI57Yfc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TDWOgpJWKQa"
      },
      "source": [
        "## Estructura del código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yEV8WbiWl6k"
      },
      "source": [
        "## Modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "teF9O9JJmG7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sDBLvbTtlwzs"
      },
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "  def fit(self, y):\n",
        "    self.names = np.unique(y) #nombres unicos\n",
        "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)} #diccionario, para cada nombre un numero\n",
        "    self.fmt = y.dtype #esto solo sirve para crear un array del mismo tipo de dato\n",
        "    \n",
        "    # Q1: why is there no need for class_to_name?\n",
        "    \"\"\"\n",
        "    Por que las tags del data set original estan definidas como nombres (strings). \n",
        "    Super ineficiente usar strings. \n",
        "    \n",
        "    //chequear si usa la clase para una operacion\n",
        "    si se usa para indexar matrices del data set que corresponden a una misma clase. \n",
        " \n",
        " \n",
        "    \"\"\"\n",
        "\n",
        "  def _map_reshape(self, f, arr):\n",
        "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "    # Q2: why is reshaping necessary?\n",
        "    \"\"\"\n",
        "    Flatten para que sea generico el metodo. \n",
        "    Para que tenga el mismo shape que el vector de etiquetas que le voy a pasar para que haga el encoder\n",
        "    al fin y al cabo lo que quiero  hacer es hacer un encoder de los nombres, no cambiar morfologia. \n",
        "    ademas ese flaten me reshapeea en una dimension renglon.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  def transform(self, y):\n",
        "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "  def fit_transform(self, y):\n",
        "    self.fit(y)\n",
        "    return self.transform(y)\n",
        "\n",
        "  def detransform(self, y_hat):\n",
        "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "m0KYC8_uSOu4"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    self.encoder = ClassEncoder()\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: what does bincount do?\n",
        "    \"\"\"\n",
        "    bincount: Counts number of occurrences of each value in array of non-negative ints.  \n",
        "    ese a_priori lo que hace es calcular las frecuencias relativas de cada clase en el dataset.  \n",
        "    o sea este estimate a priori saca basicamente lo que vendria a ser un histograma....   \n",
        "    y el bincount cuenta los elementos de y pero aplastados en 1 renglon. \n",
        "    solo acepta ints asi que debe usar algo que paso por el encoder\n",
        "\n",
        "    le pegue una chequeada y te tira esto [0.33333333 0.33333333 0.33333333]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return np.log(a_priori)\n",
        "    \n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # first encode the classes\n",
        "    y = self.encoder.fit_transform(y)\n",
        "\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # check that a_priori has the correct number of classes\n",
        "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: why do we need to do this last, can't we do it first?\n",
        "    \"\"\"\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "    sin encoder no se puede indexar con I (tendrias strings)\n",
        "    Sin estimar a priori, esta funcion tampoco se podria evaluar len(self.log_a_priori)  (si bien el assert te permitiria usar encoder names)\n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "    \n",
        "    for i in range(m_obs):\n",
        "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i \n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IRamFdiGDuSR"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate COMMON  covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: why not just X[:,y==idx]?\n",
        "    # Inv_covs tiene tres matrices de covarianza en principio, distintas.\n",
        "    # Matriz de covarianza es de 4x4 features \n",
        "    # para indexar se necesita un vector unidimensional, y el vector y tiene forma de matriz de 90x1. \n",
        "\n",
        "\n",
        "    # Q6: what does bias=True mean? why not use bias=False?\n",
        "    \n",
        "    \"\"\"\n",
        "    np.cov: Estimate a covariance matrix, given data and weights.\n",
        "    bias: Default normalization (False) is by (N - 1), where N is the number of observations given (unbiased estimate). \n",
        "    If bias is True, then normalization is by N.  1/N   \n",
        "\n",
        "    Sabemos que las poblaciones tienen distibucion normal por hipotesis\n",
        "    Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "    Y sabemos que para esos casos un estimador de maximum liklyhhod para la \n",
        "    covarianza de pobalcions que siguen distribucion normal tiene un factor de normalizacion de tipo 1/N\n",
        "\n",
        "    In cases where the distribution of the random variable X is known to be within a certain family of distributions, \n",
        "    other estimates may be derived on the basis of that assumption. A well-known instance is when the random variable X \n",
        "    is normally distributed: in this case the maximum likelihood estimator of the covariance matrix is slightly different \n",
        "    from the unbiased estimate, and is given by \n",
        "\n",
        "\n",
        "       \n",
        "    https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices\n",
        "    \"\"\"\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: what does axis=1 mean? why not axis=0 instead?\n",
        "\n",
        "    # significa que voy a calcular la media en direccion de los renglones , eporque los datos estan en filas luego de hacer \n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "    \n",
        "    # el indexado me devuelve la sub matriz que corresponde a muestras de una misma clase. y sobre eso le calculo una media\n",
        "\n",
        "    # es decir que obtenes la media de los 4 features para cada clase. el vector de medias es una lista de 3 elementos donde cada elemento tiene 4 componentes. \n",
        "\n",
        "    \n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_zoK-gWkRf"
      },
      "source": [
        "## Código para pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m05KrhUDINVs"
      },
      "outputs": [],
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6667"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkXcoldXOqs",
        "outputId": "b07a5027-be83-4c0a-a09e-e4f3a21e4c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "X_full, y_full = get_iris_dataset()\n",
        "\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAk-UQCjKecT",
        "outputId": "ddf4e2f6-1baf-4a51-de72-5ce1d7c03db8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek data matrix\n",
        "X_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdzMURX2KVdO",
        "outputId": "66a3cd6b-7dda-4618-b13f-628d113bf7d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa']], dtype='<U10')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek target vector\n",
        "y_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP_QmWCIECs",
        "outputId": "36c28bcc-5d33-43e6-f231-3f3bf7b460cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "# preparing data, train - test validation\n",
        "# 70-30 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "\n",
        "# traspose everything because this model format assumes column vectors\n",
        "train_x = X_train.T\n",
        "train_y = y_train.T\n",
        "test_x = X_test.T\n",
        "test_y = y_test.T\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dGIf2TA5SpoT"
      },
      "outputs": [],
      "source": [
        "qda = QDA()\n",
        "\n",
        "qda.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Q30DyLWpTL",
        "outputId": "c113c448-5230-44be-8f85-7a6d3f732d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0000 while test error is 0.0333\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yb1V7_yXRfO"
      },
      "source": [
        "# Consigna\n",
        "\n",
        "## Implementación\n",
        "1. Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias?¿Por qué?\n",
        "2. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
        "1. *(Opcional)* En `BaseBayesianClassifier._predict_one` se estima la posteriori de cada clase por separado, a pesar de que la cuenta es siempre la misma (cambiando valores de parámetros, pero no dimensiones). Aprovechando el *broadcasting* de NumPy, se puede reemplazar ese list comprehension por un cálculo *tensorizado* donde tanto medias como varianzas (o inversas) estén \"stackeadas\" sobre un tercer eje, permitiendo el cálculo en paralelo de todas las clases con un:\n",
        "`log_posteriori = self.tensor_log_a_priori + self._predict_log_conditionals(x)`. Implementar dicha modificación y mostrar su uso. *Ayuda: los métodos `np.stack` y la documentación del operador [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) son de gran utilidad.*\n",
        "\n",
        "## Preguntas técnicas\n",
        "\n",
        "Responder las 7 preguntas que se encuentran distribuidas a lo largo del código.\n",
        "\n",
        "## Preguntas teóricas\n",
        "\n",
        "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
        "\n",
        "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
        "\n",
        "3.La implementación de QDA estima la probabilidad condicional utilizando 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x que no es exactamente lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preguntas teoricas\n",
        "\n",
        "### 1- \n",
        "\n",
        "Dado que para LDA $\\Sigma_j == \\Sigma $:\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva. Luego, distribuyendo las matrices resultantes:\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2} x^T \\Sigma^{-1} x + \\frac{1}{2} x^T \\Sigma^{-1} \\mu_j + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C \n",
        "$$\n",
        "\n",
        "El primer término de la suma es independiente de la clase j, por lo que se puede incorporar a la constante aditiva.\n",
        "Ahora, si tomamos el segundo término de la suma, usando la propiedad de la transpuesta de un producto de matrices $ (AB)^T = B^T A^T $ y que $ (A^T)^T = A $ , resulta que \n",
        "$$\n",
        "x^T \\Sigma^{-1} \\mu_j = ( \\mu_j^T (\\Sigma^{-1})^T x )^T\n",
        "$$\n",
        "Dado que la matríz $\\Sigma$ es cuadrada y simétrica, luego $\\Sigma ^ -1$ es cuadrada y simétrica, entonces $ (\\Sigma^{-1})^T = (\\Sigma^{-1})$. Por lo tanto, la ecuación para el logaritmo de la probabilidad resulta:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\frac{1}{2} [\\mu_j^T \\Sigma^{-1} x ]^T + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C \n",
        "$$\n",
        "\n",
        "Ahora, viendo las dimensiones de las matrices del primer término, tenemos que $\\mu_j \\epsilon ℝ^{(1xk)}$,  $\\Sigma \\epsilon ℝ^{(kxk)}$ y  $x \\epsilon ℝ^{(kx1)} $, donde k es la cantidad de features del dataset.\n",
        "\n",
        "Por lo tanto $ [\\mu_j^T \\Sigma^{-1} x ] $ es un escalar y la ecuación se puede reducir de la siguiente forma:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C = \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j^T) + C\n",
        "$$\n",
        "\n",
        "Lo que resulta que, para LDA:\n",
        "$$\n",
        "\\log{f_j(x)} = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j^T) + C\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 2-\n",
        "\n",
        "Como se demostro anteriormente, la funcion a maximizar de LDA tiene un termino lineal. \n",
        "\n",
        "LDA tiene solo terminos lineales en su funcion a maximizar. \n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "Sin embargo \n",
        "\n",
        "QDA tiene un termino cuadratico, que no se puede simplificar como en LDA porque la matriz de covarianza depende de la clase\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "\n",
        "### 3-  Propiedades de los logaritmos\n",
        "\n",
        "teoria\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "Codigo\n",
        "`0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x`\n",
        "\n",
        "\n",
        "Respuesta:el codigo usa la inversa de matriz de covarianza, mientras que el apartado teorico no. \n",
        "\n",
        "Estos dos valores son equivalentes porque al -log(det(v))  = log( det( inv(v) ) ), esto es por propiedades del logartimo para los exponentes, para la funcion log baja el - 1. \n",
        "//redactar mejor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## implementacion punto 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## implementacion punto 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "1ChynN-GXSL5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate COMMON  covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True)) \n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    \n",
        "\n",
        "    total = len(y)  # ejemplo 90\n",
        "    (unique, counts) = np.unique(y, return_counts=True)  # (0, cantidad de ceros) (1, cantidad de 1) (2, cantidad de 2)\n",
        "    self.inv_cov_inbetween = np.zeros(self.inv_covs[0].shape)\n",
        "\n",
        "    #asumo que las matrices de covariaza estan ordenadas por clase. entonces\n",
        "    for index in range(len(counts)):\n",
        "      self.inv_cov_inbetween = self.inv_cov_inbetween + np.multiply( self.inv_covs[index], (counts[index])/total )\n",
        "\n",
        "    print(np.shape(self.inv_cov_inbetween))\n",
        "    self.inv_covs = [self.inv_cov_inbetween for i in range(len(unique))]\n",
        "\n",
        "\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(det(np.linalg.inv( inv_cov))) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 4)\n"
          ]
        }
      ],
      "source": [
        "un_lda = LDA()\n",
        "\n",
        "un_lda.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparacion de metodos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDA Train (apparent) error is 0.00000000 while test error is 0.03333333\n",
            "LDA Train (apparent) error is 0.02222222 while test error is 0.03333333\n"
          ]
        }
      ],
      "source": [
        "def accuracy_qda(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"QDA Train (apparent) error is {1-train_acc:.8f} while test error is {1-test_acc:.8f}\")\n",
        "\n",
        "\n",
        "def accuracy_lda(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, un_lda.predict(train_x))\n",
        "test_acc = accuracy(test_y, un_lda.predict(test_x))\n",
        "print(f\"LDA Train (apparent) error is {1-train_acc:.8f} while test error is {1-test_acc:.8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
        "\n",
        "Se usaron valores de random seed:  0 , 42,  425, 6543, 888, 6667\n",
        "\n",
        "Los valores obtenidos fueron: \n",
        "\n",
        "\n",
        "\n",
        "Puede observarse que tanto los modelos lineales como cuadraticos tienen el mismo error de test en ambos casos. pero distintos errores de test"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AMIA - TP final LDA/QDA.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c28c3cf458679f9b83f61209d65f8a8f2d99f99372cb71f1948565dfdc4b7b2a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('FIUBA001')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
